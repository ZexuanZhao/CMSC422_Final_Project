---
title: "Random forest"
author: "Zexuan Zhao"
date: "2022-12-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(parallel)
library(tidyverse)
```

```{r include = FALSE}
## Util

## Extract false positive rate from rf
false_positive <- function(rf){
  rf$confusion["no", "class.error"]
}
## Extract false negative rate from rf
false_negative <- function(rf){
  rf$confusion["yes", "class.error"]
}
## Extract error rate from rf
error_rate <- function(rf){
  rf$err.rate[,1] %>% mean()
}
```

# Replicate original random forest analysis in the paper

The followings are codes modified from `scripts_from_original_paper/7_Random_forest/random_forest_NEW_3.r`.

## Load CSV data and exclude redundant variables

```{r}
data <- read_csv("../data/AppendixS1.csv") %>% 
# Exclude low-level taxonomic variables
  select(-Species, -Genus, -Family, -Order) %>% 
# Exclude redundant statistical variables
  select(-pmtr, -dgr, -dgp, -der, -dep, -ger, -gep) %>% 
# Exclude variables of genetic sequence
  select(-bp, -pi) %>% 
# Exclude variables generated by downstream analysis 
  select(-r, -c, -t, -p,	-f,	-fp,	-cgeo,	-tgeo,	-pgeo,	-cenv,	-tenv,	-penv) %>% 
# Exclude rows with NAs
  na.omit()
dim(data)
```

In total there are `r ncol(data)-1` predictors, 1 response variable and `r nrow(data)` data points.

## Convert p-value to labels

`pmtp` is generated by `mantel.partial(mydata.dnadist, mydata.geodist, mydata.envdist, method = "pearson", permutations = 999)` from `scripts_from_original_paper/8_IBE/IBE.r`.

```{r}
data_labeled <- data %>% 
# Convert p-value to reject/accpet null hypothesis by a threshold of 0.05
  mutate(pmtp = ifelse(pmtp <= 0.05, "yes", "no") %>% as.factor())
```

The data set is highly unbalanced because the positive cases are only `r (data_labeled %>% filter(pmtp == "yes") %>% nrow())/nrow(data_labeled)*100`%.

```{r}
# Calculate percentage of yes/no
data_labeled %>% 
  group_by(pmtp) %>% 
  summarize(n = n(), p = n/12252)
```

## Run random forest and imporance analysis

```{r eval = FALSE}
rf_unfiltered_unbalanced <- randomForest(pmtp ~ ., 
                   data=data_labeled, 
                   ntree=1000, importance=TRUE, nPerm=100)
rf_unfiltered_unbalanced
#saveRDS(rf_unfiltered_unbalanced, "rf_unfiltered_unbalanced.RDS")
```

```{r echo = FALSE}
if (!exists("rf_unfiltered_unbalanced")){
  rf_unfiltered_unbalanced <- readRDS("rf_unfiltered_unbalanced.RDS")
}
rf_unfiltered_unbalanced
```

If no filtering and balancing is applied, the overall accuracy is relatively high (`r rf_unfiltered_unbalanced %>% error_rate()`) but highly biased (false negative rate = `r rf_unfiltered_unbalanced %>% false_negative()` ). See important variables:

```{r}
importance(rf_unfiltered_unbalanced) %>%
  as_tibble(rownames = "var") %>% 
# Sort by MDA descendantly
  arrange(desc(MeanDecreaseAccuracy)) %>% 
  select(var, MeanDecreaseAccuracy)
```

## Re-run random forest using balanced data and set `n >= 20`

Apparently `n` is an important predictor, but it is an artifect that has nothing to do with the scientific question I'm addressing. `n` is the number of individuals whose genetic sequences were used to infer genetic structure. `n` too low means the inference of genetic structure is not reliable. In the paper, they claimed that `n>=20` is a good threshold.

```{r}
data_labeled_20 <- data_labeled %>% 
  filter(n >= 20)
dim(data_labeled_20)
```

After applying `n >= 20`, there are only `r nrow(data_labeled_20)` (`r nrow(data_labeled_20)/nrow(data)`%) data points left. See how filtering by `n >=20` changes accuracy:

```{r eval = FALSE}
rf_filtered_unbalanced <- randomForest(pmtp ~ ., 
                                       data=data_labeled_20, 
                                       ntree=1000, importance=TRUE, nPerm=100)
rf_filtered_unbalanced
saveRDS(rf_filtered_unbalanced, "rf_filtered_unbalanced.RDS")
```

```{r echo = FALSE}
if (!exists("rf_filtered_unbalanced")){
  rf_filtered_unbalanced <- readRDS("rf_filtered_unbalanced.RDS")
}
rf_filtered_unbalanced
```

As shown above, the model is still predicting badly on positive cases. What's more, it's predicting worse on true negative cases and the overall error rate is even higher. It could be because of the low number of positive cases in the training data set and applying filter makes both positive cases and negative cases rarer, so the model does not have much data to train. We can (partially) solve this problem by resampling from the original data set and sample equal numbers of positive and negative cases.

```{r}
# Sample from original data set to get balanced data set. 
# By default the resampled data set will be 2 times the size of the original one.
# Return the resampled dataframe
generate_balanced_data <- function(data, N = 2*nrow(data)){
  data_pos <- data %>% 
    filter(pmtp == "yes")
  data_neg <- data %>% 
    filter(pmtp == "no")
  data_pos_sample <- data_pos %>% 
    slice_sample(n = round(N/2), replace = TRUE)
  data_neg_sample <- data_neg %>% 
    slice_sample(n = N - round(N/2), replace = TRUE)
  data_pos_sample %>% 
    bind_rows(data_neg_sample)
}
```

See how balanced training data set can affect accuracy:

```{r eval=FALSE}
rf_unfiltered_balanced <- randomForest(pmtp ~ ., 
                                       data=generate_balanced_data(data_labeled), 
                                       ntree=1000, importance=TRUE, nPerm=100)
saveRDS(rf_unfiltered_balanced, "rf_unfiltered_balanced.RDS")
```

```{r echo = FALSE}
if (!exists("rf_unfiltered_balanced")){
  rf_unfiltered_balanced <- readRDS("rf_unfiltered_balanced.RDS")
}
rf_unfiltered_balanced
```

As shown above, the accuracy is elevated so much and the bias is also eliminated!

Putting it together, I made a pipeline function that does balancing sampling, random forest building and importance analysis:

```{r}
# Run random forest and importance analysis once on resampled balanced data.
# Return a dataframe merging the error rate from rf and MeanDecreaseAccuracy from imp
random_forest_importance_once <- function(i, data){
  rf <- randomForest(pmtp ~ ., 
                     data = generate_balanced_data(data), 
                     ntree=1000, importance=TRUE, nPerm=100)
  error_rate <- rf$confusion[,3] %>% as_tibble_row() %>% 
    mutate(index = i) %>% 
    gather(key = "type", value = "value", -index)
  imp <- importance(rf2) %>%
    as_tibble(rownames = "var") %>% 
    arrange(desc(MeanDecreaseAccuracy)) %>% 
    select(var, MeanDecreaseAccuracy) %>% 
    mutate(index = i) %>% 
    select(index, type = var, value = MeanDecreaseAccuracy)
  error_rate %>% 
    bind_rows(imp)
}
```

Here I did resampling 100 times, pooled the results and only showed the mean of all metrics.

```{r eval = FALSE}
# Use parallel computing in R
random_forests <- mclapply(1:100, 
                           random_forest_importance_once, 
                           data = data_labeled_20, 
                           mc.cores = 6)
random_forests_tbl <- do.call(bind_rows, random_forests)
#saveRDS(random_forests_tbl, "random_forests_tbl.RDS")
```

```{r include = FALSE}
if (!exists("random_forests_tbl")){
  random_forests_tbl <- readRDS("random_forests_tbl.RDS")
}
```

Summarizing the error rates from 100 resamplings

```{r}
imp_filtered_balanced <- random_forests_tbl %>% 
  filter(type == "yes" | type == "no") %>% 
  group_by(type) %>% 
  summarize(mean_error = mean(value))
imp_filtered_balanced
```

Summarizing the mean decrease accuracy of predictors from 100 resamplings

```{r}
MDA <- random_forests_tbl %>% 
  filter(type != "yes" & type != "no") %>% 
  group_by(type) %>% 
  summarize(meanDecreaseAccuracy = mean(value)) %>% 
  arrange(desc(meanDecreaseAccuracy))
saveRDS(MDA, "MDA.RDS")
```

Comparing accuracies based on four data set:

```{r}
summary_accuracy <- 
rf_unfiltered_unbalanced$confusion[,3] %>% 
  as_tibble_row() %>% 
  gather(key = true, value = unfiltered_unbalanced) %>% 
  left_join(rf_unfiltered_balanced$confusion[,3] %>% 
              as_tibble_row() %>% 
              gather(key = true, value = unfiltered_balanced)
            ) %>% 
  left_join(rf_filtered_unbalanced$confusion[,3] %>% 
              as_tibble_row() %>% 
              gather(key = true, value = filtered_unbalanced)
            ) %>% 
  left_join(imp_filtered_balanced %>% 
              select(true = type, filtered_balanced = mean_error)
            )
saveRDS(summary_accuracy, "summary_accuracy_randomForest.RDS")
summary_accuracy
```

Plotting the error rate with combination of filtered and balanced

```{r}
summary_accuracy_plot <- summary_accuracy %>% 
  gather(key = type, value = error, -true) %>% 
  separate(type, into = c("filtered", "balanced"), sep = "_") %>% 
  mutate(grouper = interaction(filtered, balanced)) %>% 
  group_by(grouper) %>% 
  ggplot(aes(x = filtered, y = error, group = grouper, fill = true)) +
    geom_bar(stat = "identity",
             position = "dodge") +
  facet_grid(. ~ balanced) +
  theme_bw() +
  xlab(NULL)
summary_accuracy_plot
ggsave("../report/img/summary_accuracy_plot.png", 
       plot = summary_accuracy_plot, 
       width = 4, height = 3, units = "in")
```
